{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c06354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It stopped working, it loads for a little bit....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've not been able to run this app so my revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's surprising how bad the app is compared to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A colossal waste of time. Somehow, the prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It looks cool but it just doesn't work on my p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55817</th>\n",
       "      <td>üëçüëç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55818</th>\n",
       "      <td>üòçüòçüòç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55819</th>\n",
       "      <td>üíôüíôüíôüíôüíô</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55820</th>\n",
       "      <td>üëçüëçüëçüëç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55821</th>\n",
       "      <td>üî•üî•üî•üî•üî•üî•üî•üî•</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55822 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content\n",
       "0      It stopped working, it loads for a little bit....\n",
       "1      I've not been able to run this app so my revie...\n",
       "2      It's surprising how bad the app is compared to...\n",
       "3      A colossal waste of time. Somehow, the prompts...\n",
       "4      It looks cool but it just doesn't work on my p...\n",
       "...                                                  ...\n",
       "55817                                                 üëçüëç\n",
       "55818                                                üòçüòçüòç\n",
       "55819                                              üíôüíôüíôüíôüíô\n",
       "55820                                               üëçüëçüëçüëç\n",
       "55821                                           üî•üî•üî•üî•üî•üî•üî•üî•\n",
       "\n",
       "[55822 rows x 1 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('C:/Users/asus/Documents/2.csv', usecols=['content'])#mention the name of file to be extracted\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a3f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['totalwords'] = df['content'].str.split().str.len() # Calculate word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd896a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc[df['totalwords'] > 5] #Removing all reviews having less than two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f59ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('totalwords',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b72eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[data['content'].map(lambda x: x.isascii())] #Removing all non-english reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e972c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='all') # dropping blank rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46ca469",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"New.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334f1744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It stopped working, it loads for a little bit....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've not been able to run this app so my revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's surprising how bad the app is compared to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A colossal waste of time. Somehow, the prompts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It looks cool but it just doesn't work on my p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55078</th>\n",
       "      <td>Me a ayudado mucho y si bien la calidad de res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55079</th>\n",
       "      <td>Muy bien!!! Va igual de bien que en la web!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55089</th>\n",
       "      <td>Kya batae fan ho geye iska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55135</th>\n",
       "      <td>Chat GPT menurut saya adalah AI terbaik sejauh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55702</th>\n",
       "      <td>\"ChatGPT: Adbhut Vichar-Vyakti! Samarthan, Sam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25710 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content\n",
       "0      It stopped working, it loads for a little bit....\n",
       "1      I've not been able to run this app so my revie...\n",
       "2      It's surprising how bad the app is compared to...\n",
       "3      A colossal waste of time. Somehow, the prompts...\n",
       "4      It looks cool but it just doesn't work on my p...\n",
       "...                                                  ...\n",
       "55078  Me a ayudado mucho y si bien la calidad de res...\n",
       "55079      Muy bien!!! Va igual de bien que en la web!!!\n",
       "55089                         Kya batae fan ho geye iska\n",
       "55135  Chat GPT menurut saya adalah AI terbaik sejauh...\n",
       "55702  \"ChatGPT: Adbhut Vichar-Vyakti! Samarthan, Sam...\n",
       "\n",
       "[25710 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c545f627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx = df['content'].to_string(header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1223eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Example string with weird font characters plus an URL which we gonna remove.\n",
    "sample =  xx\n",
    "\n",
    "sample = remove_url(sample)\n",
    "#print(f\"Text after removing url:- \\n {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bebc655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demoji in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install demoji\n",
    "import demoji\n",
    "\n",
    "\n",
    "def handle_emoji(string):\n",
    "    emojis = demoji.findall(string)\n",
    "\n",
    "    for emoji in emojis:\n",
    "        string = string.replace(emoji, \" \" + emojis[emoji].split(\":\")[0])\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "#print(f\"Before Handling emoji:- \\n {sample}\")\n",
    "#print(f\"After Handling emoji:- \\n {handle_emoji(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90e7047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "sample = sample\n",
    "#print(sample)\n",
    "#print(word_tokenizer(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a6f1808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "en_stopwords.update(('chatgpt','chatgpt ','CHATGPT'))#Words to be ommitted \n",
    "#print(f\"Stop Words in English : \\n{ en_stopwords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52435653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in en_stopwords]\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "#print(f\"Before removing stopwords : {word_tokenizer(sample)}\")\n",
    "#print(f\"After removing stopwords : {remove_stopwords(word_tokenizer(sample))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2657fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text): #TO CONVERT WORDS IN THEIR BASE FORM IE RAN, RUNNIN, RUN TO RUN\n",
    "\n",
    "    # text = [sp(word).lemma_ for word in text]\n",
    "\n",
    "    text = \" \".join(text)\n",
    "    token = sp(text)\n",
    "\n",
    "    text = [word.lemma_ for word in token]\n",
    "    return text\n",
    "\n",
    "\n",
    "#print(f\"Before Lemmatization : {word_tokenizer(sample)}\")\n",
    "#print(f\"After Lemmatization : {lemmatization(word_tokenizer(sample))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4d1f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    \n",
    "    text = remove_url(text) \n",
    "    text = handle_emoji(text)\n",
    "    text = text.lower() \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = word_tokenizer(text)\n",
    "    # text = stemming(text)\n",
    "    text = lemmatization(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1546873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "625a4865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25710/25710 [02:44<00:00, 156.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "df['content'] = df['content'].progress_map(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "154da3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gpt.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84eeb48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', 'work', 'load', 'little', 'bit', 'soon', 'start', 'type', 'start', 'load', 'know', 'circle', 'turn', 'type', 'box', 'go', 'away', 'completely', 'unusable', 'type', 'anything', 'anymore']\n"
     ]
    }
   ],
   "source": [
    "import gensim #Topic Modelling Technicque called LDA (Latent Dirichlet Allocation)\n",
    "from gensim.utils import simple_preprocess\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data = df['content']\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b63845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b110cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=50, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c70ca860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['chatgpt'])\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "362b7137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['work', 'load', 'little', 'bit', 'soon', 'type', 'start', 'load', 'type', 'box', 'away', 'completely', 'unusable', 'type', 'anymore']]\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "import spacy\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ','ADV'])\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc26866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (2, 1), (3, 1), (4, 1), (6, 1), (9, 1), (10, 2), (11, 1), (12, 1), (15, 3), (16, 1), (17, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4c259ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e4745ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics= 3, #change it 1-10\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2e568c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.367924299691438\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score for topic 2\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246d65c",
   "metadata": {},
   "source": [
    "## from pprint import pprint\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5122ef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 0 elements, new values have 3 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m     sent_topics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([sent_topics_df, contents], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(sent_topics_df)\n\u001b[1;32m---> 25\u001b[0m df_topic_sents_keywords \u001b[38;5;241m=\u001b[39m \u001b[43mformat_topics_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mldamodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_lemmatized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Format\u001b[39;00m\n\u001b[0;32m     28\u001b[0m df_dominant_topic \u001b[38;5;241m=\u001b[39m df_topic_sents_keywords\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "Cell \u001b[1;32mIn[32], line 18\u001b[0m, in \u001b[0;36mformat_topics_sentences\u001b[1;34m(ldamodel, corpus, texts)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[38;5;66;03m#sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43msent_topics_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDominant_Topic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerc_Contribution\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic_Keywords\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Add original text to the end of the output\u001b[39;00m\n\u001b[0;32m     20\u001b[0m contents \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(texts)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:6002\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   6000\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   6001\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[1;32m-> 6002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6003\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m   6004\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\properties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:730\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    729\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:225\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\base.py:70\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 0 elements, new values have 3 elements"
     ]
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts= data_lemmatized)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1e5f77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv(\"Q2 Single words Group.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "967c4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_lemmatized for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41cbf10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Q2 Groups Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79de0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
